{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Word Tokenizer"
      ],
      "metadata": {
        "id": "K0eSJyMEhzuj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUafbD6yhhry",
        "outputId": "70173ed0-290e-4834-871a-a239ef1c17c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['Hello', ',', 'world', '!', 'This', 'is', 'a', 'test', 'sentence', 'for', 'word', 'tokenization', '.', 'English', 'grammar', 'has', 'four', 'sentence', 'structures', ':', 'simple', ',', 'compound', ',', 'complex', 'and', 'compound-complex', '.', 'In', 'this', 'lesson', ',', 'you', '’', 'll', 'learn', 'about', 'simple', 'sentences', ',', 'but', 'first', ',', 'think', 'of', 'your', 'favourite', 'food', '.', 'Now', 'imagine', 'eating', 'that', 'food', 'every', 'night', 'for', 'a', 'year', '.', 'Would', 'it', 'still', 'be', 'your', 'favourite', 'food', 'at', 'the', 'end', 'of', 'that', 'time', '?', 'Probably', 'not', '.', 'By', 'then', 'you', '’', 'd', 'be', 'tired', 'or', 'bored', 'with', 'it', ',', 'right', '?', 'The', 'same', 'logic', 'applies', 'to', 'writing', '.', 'If', 'you', 'read', 'the', 'same', 'type', 'of', 'sentence', 'over', 'and', 'over', 'again', ',', 'you', '’', 'd', 'become', 'tired', 'of', 'it', ',', 'just', 'like', 'you', '’', 'd', 'become', 'tired', 'of', 'eating', 'the', 'same', 'food', 'over', 'and', 'over', 'again', '.', 'That', '’', 's', 'why', 'good', 'English', 'writers', 'use', 'all', 'four', 'types', 'of', 'sentences', ',', 'not', 'just', 'one', '.', 'That', '’', 's', 'also', 'why', 'it', '’', 's', 'important', 'for', 'you', 'to', 'be', 'able', 'to', 'write', 'each', 'type', 'correctly', '.']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "def tokenize_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    return tokens\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    text = \"Hello, world! This is a test sentence for word tokenization. English grammar has four sentence structures: simple, compound, complex and compound-complex. In this lesson, you’ll learn about simple sentences, but first, think of your favourite food. Now imagine eating that food every night for a year. Would it still be your favourite food at the end of that time? Probably not. By then you’d be tired or bored with it, right? The same logic applies to writing. If you read the same type of sentence over and over again, you’d become tired of it, just like you’d become tired of eating the same food over and over again. That’s why good English writers use all four types of sentences, not just one. That’s also why it’s important for you to be able to write each type correctly.\"\n",
        "    tokens = tokenize_text(text)\n",
        "    print(\"Tokens:\", tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regional Language Filteration"
      ],
      "metadata": {
        "id": "uyH6jQijiiar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langdetect"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRWsHKiYircd",
        "outputId": "7623d694-186b-4620-9d7d-46bbecafbfc8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993221 sha256=d539e17531adb7b5234bd473e6e98db8f6beda234f82046ea4f18947184cabbc\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langdetect import detect, DetectorFactory\n",
        "from langdetect.lang_detect_exception import LangDetectException\n",
        "\n",
        "DetectorFactory.seed = 0\n",
        "\n",
        "def is_marathi(word):\n",
        "    try:\n",
        "        return detect(word) == 'mr'\n",
        "    except LangDetectException:\n",
        "        return False\n",
        "\n",
        "def filter_marathi_words(text):\n",
        "    words = text.split()\n",
        "    marathi_words = [word for word in words if is_marathi(word)]\n",
        "    return marathi_words\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    text = \"आजचा दिवस खूप सुंदर आहे. The weather is perfect for a walk. आपण बाहेर जाऊन गार्डनमध्ये वेळ घालवूया. I hope you are enjoying the day as much as I am.\"\n",
        "    marathi_words = filter_marathi_words(text)\n",
        "    print(\"Marathi Words:\", marathi_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jpNP6c1jRDz",
        "outputId": "f0add44a-2375-4235-e3d5-fa9521872be2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Marathi Words: ['आजचा', 'दिवस', 'सुंदर', 'आहे.', 'आपण', 'बाहेर', 'जाऊन', 'गार्डनमध्ये', 'वेळ', 'घालवूया.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stopwords Filteration"
      ],
      "metadata": {
        "id": "cj6taRyqj_sc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "def filter_stop_words(text):\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    text = \"This is a sample sentence demonstrating stop word filtration in Python.\"\n",
        "    filtered_text = filter_stop_words(text)\n",
        "    print(\"Filtered Text:\", filtered_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfT2UdiWkEQ0",
        "outputId": "79cad700-e9f2-4a78-b92d-a936dc2c692b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Text: sample sentence demonstrating stop word filtration Python .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    translator = str.maketrans('', '', string.punctuation)  #translation table to replace each punctuatioon with ''\n",
        "    return text.translate(translator)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    text = \"Hello, world! This is a test sentence with punctuation marks.\"\n",
        "    text_without_punctuation = remove_punctuation(text)\n",
        "    print(\"Text without punctuation:\", text_without_punctuation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FFoERzOke_H",
        "outputId": "9668b1a9-2ce8-4ffd-e73f-7c8f258c2d92"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text without punctuation: Hello world This is a test sentence with punctuation marks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phone, Email, Name Validation"
      ],
      "metadata": {
        "id": "StEc2TTVk-rO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def validate_phone_number(phone_number):\n",
        "    pattern = re.compile(r'^\\+?[1-9]\\d{1,14}$')\n",
        "    return bool(pattern.match(phone_number))\n",
        "\n",
        "def validate_email(email):\n",
        "    pattern = re.compile(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$')\n",
        "    return bool(pattern.match(email))\n",
        "\n",
        "def validate_name(name):\n",
        "    pattern = re.compile(r\"^[A-Za-zÀ-ÿ][A-Za-zÀ-ÿ' -]*[A-Za-zÀ-ÿ]$\")\n",
        "    return bool(pattern.match(name))\n",
        "\n",
        "def validate_inputs(phone_number, email, name):\n",
        "    is_phone_valid = validate_phone_number(phone_number)\n",
        "    is_email_valid = validate_email(email)\n",
        "    is_name_valid = validate_name(name)\n",
        "\n",
        "    return {\n",
        "        'phone_number_valid': is_phone_valid,\n",
        "        'email_valid': is_email_valid,\n",
        "        'name_valid': is_name_valid\n",
        "    }\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    phone_number = \"+1234567890\"\n",
        "    email = \"jaybhayerutik2@gmail.com.com\"\n",
        "    name = \"John%Doe\"\n",
        "\n",
        "    results = validate_inputs(phone_number, email, name)\n",
        "    print(\"Validation Results:\", results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_BPqNEflCgr",
        "outputId": "d3fa29f8-3040-4e53-bc8e-29691061e902"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Results: {'phone_number_valid': True, 'email_valid': True, 'name_valid': False}\n"
          ]
        }
      ]
    }
  ]
}